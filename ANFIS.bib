Automatically generated by Mendeley Desktop 1.13.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@book{Olson2008,
	abstract = {This book covers the fundamental concepts of data mining, to demonstrate the potential of gathering large sets of data, and analyzing these data sets to gain useful business understanding. The book is organized in three parts. Part I introduces concepts. Part II describes and demonstrates basic data mining algorithms. It also contains chapters on a number of different techniques often used in data mining. Part III focusses on business applications of data mining. Methods are presented with simple examples, applications are reviewed, and relativ advantages are evaluated..},
	author = {Olson, David L. and Delen, Dursun},
	doi = {10.1007/978-3-540-76917-0},
	file = {:C$\backslash$:/Users/Filo/Desktop/ELMS/Advanced Data Mining Techniques 2008.pdf:pdf},
	isbn = {9783540769163},
	issn = {00093696},
	mendeley-groups = {Thesis1},
	pages = {182},
	title = {{Advanced Data Mining Techniques}},
	year = {2008}
}


@article{Cover1967,
	abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error<tex>R</tex>of such a rule must be at least as great as the Bayes probability of error<tex>R\^{}\{ast\}</tex>--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the<tex>M</tex>-category case that<tex>R\^{}\{ast\} leq R leq R\^{}\{ast\}(2 --MR\^{}\{ast\}/(M-1))</tex>, where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
	author = {Cover, T. and Hart, P.},
	doi = {10.1109/TIT.1967.1053964},
	file = {:C$\backslash$:/Users/Filo/Desktop/Study S2/KNN 1967.pdf:pdf},
	isbn = {0018-9448},
	issn = {0018-9448},
	journal = {IEEE Transactions on Information Theory},
	mendeley-groups = {Thesis1},
	number = {1},
	pages = {21--27},
	pmid = {21919855},
	title = {{Nearest neighbor pattern classification}},
	volume = {13},
	year = {1967}
}

@article{EvelynFix1951,
	author = {{Evelyn Fix}, Ph.D. and {J.L. Hodges, Jr.}, Ph.D.},
	file = {:C$\backslash$:/Users/Filo/Desktop/Study S2/KNN 1951.pdf:pdf},
	mendeley-groups = {Thesis1},
	number = {May},
	title = {{Discriminatory analysis, nonparametric discrimination: Consistency properties}},
	year = {1951}
}


@article{Hartigan1979,
	abstract = {The K-means clustering algorithm is described in detail by Hartigan (1975). An efficient version of the algorithm is presented here. The aim of the K-means algorithm is to divide M points in N dimensions into K clusters so that the within-cluster sum of squares is minimized. It is not practical to require that the solution has minimal sum of squares against all partitions, except when M, N are small and K = 2. We seek instead "local" optima, solutions such that no movement of a point from one cluster to another will reduce the within-cluster sum of squares.},
	author = {Hartigan, J. a. and Wong, M. a.},
	doi = {10.2307/2346830},
	file = {:C$\backslash$:/Users/Filo/Desktop/Study S2/hartigan\_1979\_kmeans.pdf:pdf},
	isbn = {00359254},
	issn = {00359254},
	journal = {Journal of the Royal Statistical Society},
	mendeley-groups = {Thesis1},
	number = {1},
	pages = {100--108},
	title = {{A K-Means Clustering Algorithm}},
	volume = {28},
	year = {1979}
}

@article{Lloyd1982,
	abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantiza- tion noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quantization schemes for 26 quanta, b = 1,2,...,7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
	author = {Lloyd, S.},
	doi = {10.1109/TIT.1982.1056489},
	file = {:C$\backslash$:/Users/Filo/Desktop/Study S2/lloyd57 kmeans.pdf:pdf},
	isbn = {00189448 (ISSN)},
	issn = {0018-9448},
	journal = {IEEE Transactions on Information Theory},
	mendeley-groups = {Thesis1},
	number = {2},
	pages = {129--137},
	title = {{Least squares quantization in PCM}},
	volume = {28},
	year = {1982}
}


@article{Chandankhede2012,
author = {Chandankhede, Pankaj H},
file = {:Users/arikuncoro/Documents/009\_KULIAH/tugas\_sci\_diaz\_3/B0575042212.pdf:pdf},
journal = {International Journal of Soft Computing and Engineering},
mendeley-groups = {ANFIS},
number = {2},
pages = {475--480},
title = {{Soft Computing Based Texture Classification with MATLAB Tool}},
year = {2012}
}
@misc{Cruz,
author = {Cruz, Adriano Oliveira},
file = {:Users/arikuncoro/Documents/009\_KULIAH/tugas\_sci\_diaz\_3/anfis.pdf:pdf},
mendeley-groups = {ANFIS},
pages = {1--53},
title = {{ANFIS: Adaptive Neuro-Fuzzy Inference Systems ANFIS Architecture Hybrid Learning Algorithm ANFIS as a Universal Approximatior Simulation Examples}},
url = {http://equipe.nce.ufrj.br/adriano/fuzzy/transparencias/anfis/anfis.pdf},
urldate = {May 20, 2015}
}
@article{Mohanaiah2013,
author = {Mohanaiah, P and Sathyanarayana, P and Gurukumar, L},
file = {:Users/arikuncoro/Documents/009\_KULIAH/tugas\_sci\_diaz\_3/ijsrp-p1750.pdf:pdf},
journal = {International Journal of Scientific \& Research Publication},
mendeley-groups = {ANFIS},
number = {5},
pages = {1--5},
title = {{Image Texture Feature Extraction Using GLCM Approach}},
volume = {3},
year = {2013}
}
@INPROCEEDINGS{Ruiz2004,
author = {L. A. Ruiz and A. Fdez-Sarr\'{\i}a and J. A. Recio},
file = {:Users/arikuncoro/Documents/009\_KULIAH/tugas\_sci\_diaz\_3/508.pdf:pdf},
keywords = {multiresolution analysis,texture classification,urban,vegetation,wavelets},
mendeley-groups = {ANFIS},
pages = {1682--1750},
booktitle = {International Archives of Photogrammetry and Remote Sensing},
volume = {XXXV},
title = {{Texture Feature Extraction for Classification of Remote Sensing Data using Wavelet Decomposition: A Comparative Study}},
year = {2004}
}

@article{Sorwar,
  author    = {Golam Sorwar and
               Ajith Abraham},
  title     = {{DCT} Based Texture Classification Using Soft Computing Approach},
  journal   = {CoRR},
  volume    = {cs.AI/0405013},
  year      = {2004},
  url       = {http://arxiv.org/abs/cs.AI/0405013},
  timestamp = {Mon, 05 Dec 2011 18:05:37 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/cs-AI-0405013},
  bibsource = {dblp computer science bibliography, http://dblp.org}
} 

@article{Technological2013,
author = {Zhen, Wu and Zhe, XU},
file = {:Users/arikuncoro/Documents/009\_KULIAH/tugas\_sci\_diaz\_3/CA225.pdf:pdf},
journal = {Proceedings of the 2nd ISCCCA-13},
keywords = {-image feature extraction,d center,dct coefficients,dct domain reduced image,engineering,national digital switching system,reduced matrix,sift algorithm,technological r},
mendeley-groups = {ANFIS},
pages = {261--264},
title = {{SIFT Feature Extraction Algorithm for Image in DCT Domain}},
year = {2013}
}
@misc{StackOverFlow2014,
author = {StackOverFlow},
mendeley-groups = {ANFIS},
title = {{How to Calculate The Energy and Correlation of An Image}},
url = {http://stackoverflow.com/questions/22953557/how-to-calculate-the-energy-and-correlation-of-an-image.},
urldate = {2015-05-20},
year = {2014}
}
@misc{MathWork-Docs2014,
author = {MathWork-Docs},
mendeley-groups = {ANFIS},
title = {{Neuro-Adaptive Learning and ANFIS}},
url = {http://www.mathworks.com/help/fuzzy/neuro-adaptive-learning-and-anfis.html},
urldate = {May 20, 2015},
year = {2014}
}

@misc{KTH,
author = {Mario Fritz, Eric Hayman, Barbara Caputo and Jan-Olof Eklundh},
mendeley-groups = {ANFIS},
title = {{The KTH-TIPS and KTH-TIPS2 Image Databases}},
url = {http://www.nada.kth.se/cvap/databases/kth-tips/documentation.html},
urldate = {May 20, 2015},
year = {2004}
}

@misc{Marshall2001,
author = {Marshall, Dave},
mendeley-groups = {ANFIS},
title = {{The Discrete Cosine Transform (DCT)}},
url = {http://www.cs.cf.ac.uk/Dave/Multimedia/node231.html},
urldate = {May 22, 2015},
year = {2001}
}

@INPROCEEDINGS{Avci2015, 
author={Avci, D. and Poyraz, M. and Leblebicioglu, M.K.}, 
booktitle={Signal Processing and Communications Applications Conference (SIU), 2015 23th}, 
title={An Expert System Based on Discrete Wavelet Transform - ANFIS for Acquisition and Recognition of Invariant Features from Texture Images}, 
year={2015}, 
pages={1070-1073}, 
keywords={discrete wavelet transforms;expert systems;fuzzy neural nets;fuzzy reasoning;image classification;image texture;ANFIS;Brodatz database;DWT;adaptive network based fuzzy inference system;discrete wavelet transform;document classification;expert system;invariant feature;low-level image feature;radar imagery classification;texture analysis;texture classification;texture images;texture-based image retrieval;tissue-type image;ultrasound images recognition;wavelet decomposition method;Computational modeling;Image segmentation;Integrated circuit modeling;Expert systems;adaptive network-based fuzzy inference system;discrete wavelet transform;entropy;texture images}, 
doi={10.1109/SIU.2015.7130018}, 
month={May},}

@INPROCEEDINGS{Wu2012, 
author={Chia-Hsiang Wu and Jiann-Shu Lee and Chin-Yin Shie and Mei-Yun Su}, 
booktitle={Communications and Networking in China (CHINACOM), 2012 7th International ICST Conference on}, 
title={Rock classification via a mobile device}, 
year={2012}, 
pages={663-666}, 
keywords={feature extraction;image classification;mobile handsets;neural nets;rocks;ANFIS classifier;feature extraction;mobile device;mobile environment;mobile phone;neural network;remote server;rock classification;rock image;Feature extraction;Fuzzy logic;Image color analysis;Image edge detection;Mobile handsets;Rocks;Servers;mobile phone;rock classification;ubiquitous computing}, 
doi={10.1109/ChinaCom.2012.6417566}, 
month={Aug},}

@INPROCEEDINGS{Goswami, 
author={Goswami, S. and Bhaiya, L.K.P.}, 
booktitle={Emerging Trends in Communication, Control, Signal Processing Computing Applications (C2SPCA), 2013 International Conference on}, 
title={A Hybrid Neuro-fuzzy Approach for Brain Abnormality Detection Using GLCM based Feature Extraction}, 
year={2013}, 
pages={1-7}, 
keywords={biomedical MRI;brain;cancer;computerised tomography;feature extraction;fuzzy logic;fuzzy neural nets;fuzzy reasoning;image classification;matrix algebra;medical image processing;tumours;GLCM-based feature extraction;abnormal tissues;anatomical information;artificial neural network system;brain image abnormality detection;brain image abnormality specification;brain image classification;brain image preprocessing;brain tumor detection;classification accuracy;classification sensitivity;classification specificity;data collection;explicit knowledge representation;fuzzy inference system;fuzzy logic metaphor;gray level co-occurrence matrix;hospitals;hybrid neuro-fuzzy approach;learning;medical field;neural network metaphor;patient follow-up;repository sites;treatment planning;Accuracy;Brain;Feature extraction;Fuzzy logic;Image segmentation;Magnetic resonance imaging;Tumors;Brain tumor;Classification accuracy;GLCM;Neuro fuzzy;Sensitivity;Specificity}, 
doi={10.1109/C2SPCA.2013.6749454}, 
month={Oct},}

@INPROCEEDINGS{Costianes, 
author={Costianes, P.J. and Plock, J.B.}, 
booktitle={Applied Imagery Pattern Recognition Workshop (AIPR), 2010 IEEE 39th}, 
title={Gray-level Co-Occurrence Matrices as Features in Edge Enhanced Images}, 
year={2010}, 
pages={1-6}, 
keywords={Fourier transforms;edge detection;entropy;image texture;matrix algebra;Fourier coefficients;Fourier descriptors;binary image;edge distribution;edge enhanced images;edge enhancement;entropy;feature measures;gray scale image;gray-level cooccurrence matrices;pixel distribution;texture measures;Atmospheric modeling;Image edge detection;Information filters;Nearest neighbor searches;Pixel;Training}, 
doi={10.1109/AIPR.2010.5759705}, 
ISSN={1550-5219}, 
month={Oct},}

@article{Technological2013,
author = {Zhen, Wu and Zhe, XU},
file = {:Users/arikuncoro/Library/Application Support/Mendeley Desktop/Downloaded/Zhen, Zhe - 2013 - SIFT Feature Extraction Algorithm for Image in DCT Domain.pdf:pdf},
journal = {Proceedings of the 2nd ISCCCA-13},
keywords = {-image feature extraction,d center,dct coefficients,dct domain reduced image,engineering,national digital switching system,reduced matrix,sift algorithm,technological r},
mendeley-groups = {ANFIS},
pages = {261--264},
title = {{SIFT Feature Extraction Algorithm for Image in DCT Domain}},
year = {2013}
}

@article{Karegowda2012,
author = {Karegowda, Asha Gowda and Jayaram, M. A. and Manjunath, A. S.},
file = {:Users/arikuncoro/Downloads/mv10/kmeanscascade.pdf:pdf},
journal = {International Journal of Engineering and Advanced Technology},
mendeley-groups = {ANFIS},
number = {3},
pages = {147--151},
title = {{Cascading K-means Clustering and K-Nearest Neighbor Classifier for Categorization of Diabetic Patients}},
volume = {1},
year = {2012}
}



